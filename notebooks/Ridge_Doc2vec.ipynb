{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl04QdjKGJar",
        "outputId": "aea533ce-e401-4a4a-9e8c-36d199175a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Part of Mercari assignments\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOtDXFQwLlKm",
        "outputId": "4203fe1e-0a8c-4476-b300-d7c40e82b6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE4B5T6EL0gr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def handle_missing_inplace(dataset):\n",
        "    dataset['brand_missing'] = dataset['brand_name'].isna().astype(int)\n",
        "    dataset['descp_missing'] = dataset['item_description'].isna().astype(int)\n",
        "\n",
        "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
        "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_crossvalidation_data(dataset, split=5):\n",
        "    from sklearn.model_selection import KFold\n",
        "    # y = dataset['price']\n",
        "    train_datasets = []\n",
        "    test_datasets = []\n",
        "    skf = KFold(n_splits=split, shuffle=True, random_state=402)\n",
        "    skf.get_n_splits(dataset)\n",
        "    for train_index, test_index in skf.split(dataset):\n",
        "        train = dataset.iloc[train_index]\n",
        "        validation = dataset.iloc[test_index]\n",
        "        train_datasets.append(train)\n",
        "        test_datasets.append(validation)\n",
        "    return train_datasets, test_datasets\n",
        "\n",
        "\n",
        "def lower_case_df(dataset):\n",
        "    dataset['name'] = dataset['name'].str.lower()\n",
        "    dataset['brand_name'] = dataset['brand_name'].str.lower()\n",
        "    dataset['item_description'] = dataset['item_description'].str.lower()\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def cutting(dataset, pop_brand):\n",
        "    dataset['brand_name'] = dataset['brand_name'] \\\n",
        "        .apply(lambda x: x if x in pop_brand else \"missing\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def check_dataframe_features(df, required_features, df_type):\n",
        "    for feature in required_features:\n",
        "        try:\n",
        "            assert feature in df\n",
        "        except KeyError:\n",
        "            raise KeyError(f\"{feature} not present in {df_type} dataframe\")\n",
        "\n",
        "\n",
        "def convert_dataframe_categorical(dataset, field, value_set):\n",
        "    from pandas.api.types import CategoricalDtype\n",
        "    cat_type = CategoricalDtype(\n",
        "        categories=value_set,\n",
        "        ordered=True)\n",
        "    dataset[field] = dataset[field].astype(cat_type)\n",
        "    dataset = pd.get_dummies(\n",
        "        dataset,\n",
        "        columns=[field])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def one_hot_encoding(x, allowable_set, encode_unknown=False):\n",
        "    if encode_unknown and (allowable_set[-1] is not None):\n",
        "        allowable_set.append(None)\n",
        "\n",
        "    if encode_unknown and (x not in allowable_set):\n",
        "        x = None\n",
        "\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def clean_sentence(text):\n",
        "    text = re.sub(r'(\\w+:\\/\\/\\S+)|http.+?', \"\", text)\n",
        "    text = re.sub(r'(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])', '', text.lower())\n",
        "    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
        "\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYCmT4_nMSag"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "DEFAULT_CONDITION_IDS = [1, 2, 3, 4, 5]\n",
        "DEFAULT_SHIPPING_IDS = [0, 1]\n",
        "\n",
        "\n",
        "class Naive_Featuriser(object):\n",
        "    \"\"\"\n",
        "    The Featuriser class which preprocess inputs, which can\n",
        "    then be appropriately fed into the Model class.\n",
        "\n",
        "    This class consists of pre-processes which are common for\n",
        "    both train and test data.\n",
        "\n",
        "    Here is the list of preprocesses done in this class\n",
        "    built from dataframe-\n",
        "        [1] Checks if dataframe is in appropriate format with all\n",
        "            required fields.\n",
        "        [2] Removes all missing values in the dataframe to 'missing'\n",
        "            value.\n",
        "        [3] Lower case the text fields ['name', 'brand_name',\n",
        "            'item_description'].\n",
        "        [4] We build a list of popular brands that have occured a minimum\n",
        "            of 5 times and convert the rest of brand names to 'missing'.\n",
        "        [5] Converts ['item_condition_id', 'shipping'] fields to one hot\n",
        "            encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 brand_names,\n",
        "                 item_condition_set=DEFAULT_CONDITION_IDS,\n",
        "                 shipping_set=DEFAULT_SHIPPING_IDS):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        brand_names: List[str]\n",
        "            List of all popular brand names.\n",
        "        item_condition_set: List[int]\n",
        "            List of all possible item_condition_id values\n",
        "            in request. Default to [1, 2, 3, 4, 5].\n",
        "        shipping_set: List[int]\n",
        "            List of all possible shipping values\n",
        "            in request. Default to [0, 1].\n",
        "        \"\"\"\n",
        "        self.popular_brand_names = brand_names\n",
        "        self.item_condition_set = item_condition_set\n",
        "        self.shipping_set = shipping_set\n",
        "\n",
        "    @classmethod\n",
        "    def build_from_dataframe(cls, df_train, df_test=None):\n",
        "        \"\"\"\n",
        "        Builds the featuriser using the dataframe.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        df_train: pd.Dataframe\n",
        "            The input dataframe used for building the featuriser.\n",
        "        df_test: pd.Dataframe\n",
        "            The input dataframe used for processing.\n",
        "            Default to None\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cls: Naive_Featuriser\n",
        "            Class object for featuriser\n",
        "        df_train: pd.Dataframe\n",
        "            Processed output of df_train\n",
        "        df_test: pd.Dataframe\n",
        "            Processed output of df_test\n",
        "        \"\"\"\n",
        "        required_features = [\n",
        "            \"name\", \"item_condition_id\", \"category_name\", \"brand_name\",\n",
        "            \"shipping\", \"seller_id\", \"item_description\"\n",
        "        ]\n",
        "        check_dataframe_features(df_train, required_features, \"df_train\")\n",
        "        df_train['pri_category'] = df_train['category_name'].apply(lambda x: x.split('/')[1])\n",
        "        df_train['sec_category'] = df_train['category_name'].apply(lambda x: x.split('/')[2])\n",
        "        df_train = handle_missing_inplace(df_train)\n",
        "        df_train = lower_case_df(df_train)\n",
        "        df_train[\"name\"] = df_train[\"name\"].apply(clean_sentence)\n",
        "        df_train[\"item_description\"] = df_train[\"item_description\"].apply(clean_sentence)\n",
        "        popular_brand = df_train['brand_name'] \\\n",
        "            .value_counts() \\\n",
        "            .loc[lambda x: x.index != 'missing'] \\\n",
        "            .loc[lambda x: x >= 10].to_dict()\n",
        "        df_train = cutting(df_train, popular_brand)\n",
        "        df_train = convert_dataframe_categorical(df_train, 'item_condition_id',\n",
        "                                                 DEFAULT_CONDITION_IDS)\n",
        "        df_train = convert_dataframe_categorical(df_train, 'shipping',\n",
        "                                                 DEFAULT_SHIPPING_IDS)\n",
        "\n",
        "        if df_test is not None:\n",
        "            check_dataframe_features(df_test, required_features, \"df_test\")\n",
        "            df_test['pri_category'] = df_test['category_name'].apply(lambda x: x.split('/')[1])\n",
        "            df_test['sec_category'] = df_test['category_name'].apply(lambda x: x.split('/')[2])\n",
        "\n",
        "            df_test = handle_missing_inplace(df_test)\n",
        "            df_test = lower_case_df(df_test)\n",
        "            df_train[\"name\"] = df_train[\"name\"].apply(clean_sentence)\n",
        "            df_train[\"item_description\"] = df_train[\"item_description\"].apply(clean_sentence)\n",
        "\n",
        "            df_test = cutting(df_test, popular_brand)\n",
        "            df_test = convert_dataframe_categorical(df_test,\n",
        "                                                    'item_condition_id',\n",
        "                                                    DEFAULT_CONDITION_IDS)\n",
        "            df_test = convert_dataframe_categorical(df_test, 'shipping',\n",
        "                                                    DEFAULT_SHIPPING_IDS)\n",
        "\n",
        "        return cls(popular_brand), df_train, df_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmQErBRgMk3i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "class PricePredictionModel(object):\n",
        "    def __init__(self, pri_category_f, sec_category_f, brand_name_f_1,\n",
        "                 lgbm_model_1, lgbm_model_2):\n",
        "\n",
        "        if isinstance(pri_category_f, CountVectorizer):\n",
        "            self.pri_category_feat = pri_category_f\n",
        "        else:\n",
        "            self.pri_category_feat = \\\n",
        "              CountVectorizer(**pri_category_f)\n",
        "\n",
        "        if isinstance(sec_category_f, CountVectorizer):\n",
        "            self.sec_category_feat = sec_category_f\n",
        "        else:\n",
        "            self.sec_category_feat = \\\n",
        "              CountVectorizer(**sec_category_f)\n",
        "\n",
        "        if isinstance(brand_name_f_1, LabelBinarizer):\n",
        "            self.brand_feat_1 = brand_name_f_1\n",
        "        else:\n",
        "            self.brand_feat_1 = \\\n",
        "              LabelBinarizer(**brand_name_f_1)\n",
        "\n",
        "        self.middle_tfidf = TfidfTransformer()\n",
        "\n",
        "        if isinstance(lgbm_model_1, dict):\n",
        "            self.lgbm_model_1 = \\\n",
        "              Ridge(**lgbm_model_1)\n",
        "        else:\n",
        "            self.lgbm_model_1 = lgbm_model_1\n",
        "\n",
        "        if isinstance(lgbm_model_2, dict):\n",
        "            self.lgbm_model_2 = \\\n",
        "              LGBMRegressor(**lgbm_model_2)\n",
        "        else:\n",
        "            self.lgbm_model_2 = lgbm_model_2\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, df_train, df_valid=None):\n",
        "        Y_train = np.log1p(df_train[\"price\"])\n",
        "\n",
        "        gensim_corpus = df_train['name'] \\\n",
        "                        .apply(\n",
        "                            lambda x: word_tokenize(clean_sentence(x))) \\\n",
        "                        .tolist()\n",
        "        gensim_corpus = [\n",
        "            TaggedDocument(doc, [i]) for i, doc in enumerate(gensim_corpus)\n",
        "        ]\n",
        "        self.name_feat = Doc2Vec(\n",
        "            documents=gensim_corpus, \n",
        "            vector_size=100, window=5, \n",
        "            min_count=1, dm=0, \n",
        "            negative = 0, workers=cores, \n",
        "            epoch=30)\n",
        "        X_name_train = np.array([self.name_feat.infer_vector(word_tokenize(clean_sentence(x))) for x in df_train['name']])\n",
        "        \n",
        "        print(\"Name featuriser shape\", X_name_train.shape)\n",
        "\n",
        "        X_item_condition_train = csr_matrix(\n",
        "            df_train.loc[:,\n",
        "                         'item_condition_id_1':'item_condition_id_5'].values)\n",
        "\n",
        "        X_category_train_1 = \\\n",
        "            self.pri_category_feat.fit_transform(df_train['pri_category'])\n",
        "        X_category_train_2 = \\\n",
        "            self.sec_category_feat.fit_transform(df_train['sec_category'])\n",
        "\n",
        "        X_brand_train_1 = \\\n",
        "            self.brand_feat_1.fit_transform(df_train['brand_name'])\n",
        "\n",
        "        X_shipping_train = csr_matrix(\n",
        "            df_train.loc[:, 'shipping_0':'shipping_1'].values)\n",
        "\n",
        "        gensim_corpus = df_train['item_description'] \\\n",
        "                        .apply(\n",
        "                            lambda x: word_tokenize(clean_sentence(x))) \\\n",
        "                        .tolist()\n",
        "        gensim_corpus = [\n",
        "            TaggedDocument(doc, [i]) for i, doc in enumerate(gensim_corpus)\n",
        "        ]\n",
        "        self.item_descp_feat = Doc2Vec(\n",
        "            documents=gensim_corpus, \n",
        "            vector_size=200, window=5, \n",
        "            min_count=1, dm=0, \n",
        "            negative = 0, workers=cores, \n",
        "            epoch=30)\n",
        "        X_item_descp_train = np.array([self.item_descp_feat.infer_vector(word_tokenize(clean_sentence(x))) for x in df_train['item_description']])\n",
        "        print(\"Description featuriser shape\", X_item_descp_train.shape)\n",
        "\n",
        "        X_missing_train = csr_matrix(\n",
        "            df_train.loc[:, ['brand_missing', 'descp_missing']].values)\n",
        "        X_train = hstack(\n",
        "            (X_name_train, X_item_condition_train, \n",
        "             X_category_train_1, X_category_train_2,\n",
        "             X_brand_train_1,\n",
        "             X_shipping_train,\n",
        "             X_item_descp_train,\n",
        "             X_missing_train\n",
        "            )).tocsr()\n",
        "        print(\"train input shape\", X_train.shape)\n",
        "\n",
        "        # self.lgbm_model = self.lgbm_model.fit(X_train, Y_train)\n",
        "        # pred_train = 0.53 * self.lgbm_model.predict(X_train)\n",
        "\n",
        "        self.lgbm_model_1 = \\\n",
        "          self.lgbm_model_1.fit(X_train, Y_train) \n",
        "        pred_train = self.lgbm_model_1.predict(X_train)\n",
        "\n",
        "        # self.lgbm_model_2 = \\\n",
        "        #   self.lgbm_model_2.fit(X_train, Y_train) \n",
        "        # pred_train += 0.47*self.lgbm_model_2.predict(X_train)\n",
        "\n",
        "        train_error = mean_squared_error(Y_train, pred_train, squared=False)\n",
        "        print(f\"Train Error - {train_error:5.3f}\")\n",
        "        if df_valid is not None:\n",
        "            Y_val = np.log1p(df_valid[\"price\"])\n",
        "\n",
        "            X_name_val = np.array([self.name_feat.infer_vector(word_tokenize(clean_sentence(x))) for x in df_valid['name']])\n",
        "        \n",
        "            # assert X_name_val.shape[1] == X_name_train.shape[1]\n",
        "\n",
        "            X_item_condition_val = csr_matrix(\n",
        "                df_valid.loc[:, 'item_condition_id_1':'item_condition_id_5'].\n",
        "                values)\n",
        "            # assert X_item_condition_val.shape[1] == X_item_condition_train.shape[1]\n",
        "\n",
        "            X_category_val_1 = \\\n",
        "                self.pri_category_feat.transform(df_valid['pri_category'])\n",
        "            X_category_val_2 = \\\n",
        "                self.sec_category_feat.transform(df_valid['sec_category'])\n",
        "            # assert X_category_val_1.shape[1] == X_category_train_1.shape[1]\n",
        "            # assert X_category_val_2.shape[1] == X_category_train_2.shape[1]\n",
        "\n",
        "            X_brand_val_1 = \\\n",
        "                self.brand_feat_1.transform(df_valid['brand_name'])\n",
        "            # assert X_brand_val_1.shape[1] == X_brand_train_1.shape[1]\n",
        "            # assert X_brand_val_2.shape[1] == X_brand_train_2.shape[1]\n",
        "\n",
        "            X_shipping_val = csr_matrix(\n",
        "                df_valid.loc[:, 'shipping_0':'shipping_1'].values)\n",
        "            # assert X_shipping_val.shape[1] == X_shipping_train.shape[1]\n",
        "\n",
        "            X_item_descp_val = np.array([self.item_descp_feat.infer_vector(word_tokenize(clean_sentence(x))) for x in df_valid['item_description']])\n",
        "            # assert X_item_descp_val.shape[1] == X_item_descp_train.shape[1]\n",
        "\n",
        "            X_missing_val = csr_matrix(\n",
        "                df_valid.loc[:, ['brand_missing', 'descp_missing']].values)\n",
        "            # assert X_missing_val.shape[1] == X_missing_train.shape[1]\n",
        "\n",
        "            X_val = hstack(\n",
        "                (X_name_val, X_item_condition_val,\n",
        "                 X_category_val_1, X_category_val_2,\n",
        "                 X_brand_val_1,\n",
        "                 X_shipping_val,\n",
        "                 X_item_descp_val,\n",
        "                 X_missing_val\n",
        "                 )).tocsr()\n",
        "            # assert X_val.shape[1] == X_train.shape[1]\n",
        "\n",
        "            pred_val = self.lgbm_model_1.predict(X_val)\n",
        "            # pred_val += 0.47*self.lgbm_model_2.predict(X_val)\n",
        "\n",
        "            val_error = mean_squared_error(Y_val, pred_val, squared=False)\n",
        "            print(f\"Validation Error - {val_error:5.3f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5EJFwtMT8wG",
        "outputId": "932ef1fc-937f-4bc5-ab65-c1200c4f7cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0 \n",
            "\n",
            "Name featuriser shape (45600, 100)\n",
            "Description featuriser shape (45600, 200)\n",
            "train input shape (45600, 623)\n",
            "Train Error - 0.573\n",
            "Validation Error - 0.574 \n",
            "\n",
            "Fold 1 \n",
            "\n",
            "Name featuriser shape (45600, 100)\n",
            "Description featuriser shape (45600, 200)\n",
            "train input shape (45600, 623)\n",
            "Train Error - 0.574\n",
            "Validation Error - 0.573 \n",
            "\n",
            "Fold 2 \n",
            "\n",
            "Name featuriser shape (45600, 100)\n",
            "Description featuriser shape (45600, 200)\n",
            "train input shape (45600, 623)\n",
            "Train Error - 0.570\n",
            "Validation Error - 0.580 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "path = \"/content/gdrive/MyDrive/Mercari/Data/\"\n",
        "df_train = pd.read_csv(path+\"mercari_train.csv\")\n",
        "df_test = pd.read_csv(path+\"mercari_test.csv\")\n",
        "\n",
        "featuriser, df_train, df_test = Naive_Featuriser \\\n",
        "    .build_from_dataframe(df_train, df_test)\n",
        "\n",
        "name_args = {\n",
        "    'min_df': 2,\n",
        "    'stop_words': frozenset(['the', 'a', 'an', 'is', 'it', 'this', ]),\n",
        "    'ngram_range': (1, 2),\n",
        "    'lowercase': False\n",
        "}\n",
        "\n",
        "category_args_1 = {}\n",
        "category_args_2 = {}\n",
        "\n",
        "brand_args_1 = {'sparse_output': True}\n",
        "\n",
        "item_args = {\n",
        "    'ngram_range': (1, 3),\n",
        "    'min_df': 2,\n",
        "    'stop_words': frozenset(['the', 'a', 'an', 'is', 'it', 'this', ]),\n",
        "    'lowercase': False\n",
        "}\n",
        "\n",
        "lgbm_args_1 = {\n",
        "    'solver':\"auto\", \n",
        "    'fit_intercept':True, \n",
        "    'random_state':205\n",
        "}\n",
        "\n",
        "lgbm_args_2 = {\n",
        "    'learning_rate': 0.85,\n",
        "    'max_depth': 3,\n",
        "    'num_leaves': 110,\n",
        "    'verbosity': -1,\n",
        "    'metric': 'RMSE',\n",
        "}\n",
        "\n",
        "train_datasets, val_datasets = \\\n",
        "    build_crossvalidation_data(df_train, split=3)\n",
        "\n",
        "for i, dataset in enumerate(zip(train_datasets, val_datasets)):\n",
        "  train, val = dataset\n",
        "  print(f\"Fold {i} \\n\")\n",
        "  model = PricePredictionModel(\n",
        "      category_args_1,\n",
        "      category_args_2,\n",
        "      brand_args_1,\n",
        "      lgbm_args_1,\n",
        "      lgbm_args_2\n",
        "  )\n",
        "  model.fit(train, val)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Ridge_Doc2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
